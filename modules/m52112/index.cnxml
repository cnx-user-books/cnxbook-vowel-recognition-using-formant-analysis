<document xmlns="http://cnx.rice.edu/cnxml">

<title>Multiple Vowel Recognition Analysis</title>
<metadata xmlns:md="http://cnx.rice.edu/mdml">
  <md:content-id>m52112</md:content-id>
  <md:title>Multiple Vowel Recognition Analysis</md:title>
  <md:abstract/>
  <md:uuid>eadb18f7-2b51-406f-8006-67aef1af9151</md:uuid>
</metadata>

<content>
  <section id="eip-245"><title>The Thought Process</title><para id="eip-317"><title>Initial Considerations</title>The program we initially developed was capable of accepting one vowel sound over a course of one or two seconds. Naturally, we wished to expand our program capacity to work for sequences of vowels with variable separation in time. With the increasing complexity of our project, however, we faced even more situations and problems we had to consider. The first and most important problem we had to consider was whether or not to parse the data into sections where we would "guess" the location of the vowel. The next problem, closely connected with the first, was a method of differentiating noise from actual sound content. A last problem was implementing the method optimally into MATLAB.</para><para id="eip-979">For reasons that will become clear soon, we chose to compute vowels by creating a continuous, nonoverlapping partition of the signal instead of just separating out vowel content.
</para><para id="eip-554"><title>The Math</title>Consider the sample signal below. The actual vowel content is contained within samples 2000-3500. White Gaussian noise has been added between samples ~1000-4500. This superposition represents a combination of both external noise infiltrating the signal and unwanted content in the speech itself. To clarify that last part, consider the following example. In determining the vowel in "hat," we must find a way to deal with 1) external noise, 2) the consonants 'h' and 't', and 3) the transition between a consonant and the vowel and vice-versa.</para><para id="eip-172"><figure id="fig7">
  <media id="samp-sig" alt="problematic noise with hat">
    <image mime-type="image/jpeg" src="../../media/sampsig.jpg"/>
  </media>
  <caption>
    Sample Signal of the Word 'Hat' with Problematic Noise
  </caption>
</figure></para></section><para id="delete_me">If we go the path of taking the chunk between samples 1000 and 4000 and guessing it's the vowel (which is the best we can do in the "parsing" case), we take into consideration a lot of unwanted noise that will affect our formant guesses. Since roughly half the power in this signal is noise, our results may be corrupted. How might we get around this?</para><para id="eip-468">We looked to a partitioning method instead, because it seemed noise-effective to a higher degree. Consider taking the entire signal and dividing it into chunks of 500 samples a piece.</para><para id="eip-764">Let's say that we iterate through the sequence of chunks and put the following restriction on our method: "Whenever we see that four chunks of our signal match to the same vowel's formants, we say that vowel is definitively in the signal." If we proceed this way, we notice that samples from 1000 to 2000 will yield two formant pairs that *might* be similar, while the samples between 2000 and 4000 yield four very similar formant pairs because of the vowel content. Since the formants identified in the initial noise don't satisfy the initial restriction, we throw them out. Since the formants identified in the signal itself do satisfy the restriction, we keep them as a means to identifying the vowel. While time-"parsing" can't really deal with the complication of noise, it is easy to see that with this new "partition" method, we are effectively filtering out the effects of inconsistent noise (both external and internal) by throwing out any unreliable data.</para><para id="eip-670">This helps get rid of some of the potential inaccuracy of our program, but how might we deal with inevitable background noise? We determined experimentally that the mean (absolute value of the) amplitude of any partition containing the speech content is orders of magnitude higher than parts that are just background noise. With that, we set two conditions that would automatically exclude any such noise from being in our consideration. As we analyze the signal in progressive chunks, we first look to see if the chunk has a max amplitude of at least 0.1 of the max amplitude of the signal. Then, we check to see that the mean of the magnitude of the chunk was greater than or equal to the mean of the signal. If one or more of these conditions were not met, we would immediately discard the current chunk and move on to the next chunk of signal. 
</para><section id="eip-374"><title>The Application</title><para id="eip-892">We built the prototype by dividing our problem into five subproblems: initializing our data, reading an audio signal, determining the frequency response, cleaning up formant data and displaying information relevant to the vowels.</para><para id="eip-713"><title>Initial Data</title>In our function initialize_all_data, we store the theoretical vowel formant pairs for the words "see", "play", "hat", "palm" and "rug" into a matrix for later use. We also set up a convenient matrix called the output_matrix, created so that we could elude the need to go through 7 if-else statements when outputting a vowel.
<space>

</space></para><para id="eip-517"><title>Audio Input</title>After the user decides how long he or she wants to speak, we use a built-in MATLAB script to read in an audio signal of that length from the user. The sampling rate of 44.1kHz is intentionally high so that we can preserve a lot of information from the original signal.
</para><para id="eip-596"><title>Assumptions</title>1) The user will speak at a relatively constant amplitude. Note that this
is a little easier said than done, because words like "see" are naturally much quieter than "hat."
</para><para id="eip-463">2) The user will speak slowly and clearly so that we can look for consistency in determining the vowels said.
<space>

</space></para><para id="eip-387"><title>The Algorithm</title>1) Establish a set of variables that store theoretical vowel formants.</para><para id="eip-79">2) Record an audio signal of n seconds from the user. </para><para id="eip-726">3) Split the data into non-overlapping chunks of 4000 samples. </para><para id="eip-597">4) Preprocess the data by detrending with a low-order polynomial and using a low-order Butterworth lowpass filter. </para><para id="eip-494">5) Determine the transfer function of the vocal tract associated with the current chunk using an ar model.</para><para id="eip-98">6) Determine the peaks of the transfer function (the formants) and match
  with the closest formant, filtering by a least means-squared matched 
  filter.</para><para id="eip-577">7) If amplitude of signal exceeds 0.1 max amplitude of overall signal, we 
  assume it's potentially a vowel. Put its formants onto the "stack" of recent_formant_pairs. </para><para id="eip-432">8) If the last four formant pairs have been consistent (the same value), then we will assume the vowel estimation has worked. Add the formant pair to the "stack" of vowels identified in tracktimes_and_formants.</para><para id="eip-968">9) Now we have processed our guesses for what vowels were said when. We
  are going to have repeats, so run through a for-loop to clean up the
track_times_and_formants vector into a new, workable vector called track_begin_end_formants.</para><para id="eip-858">10) Output data and guesses. 
<space>

</space></para><para id="eip-195"><title>Limitations</title>1) Requires very clear enunciation. Difficult to establish because we naturally change the pitch of our words as we start and end them. Example: the end of the vowel in "strut" sounds remarkably similar to "hat" because of the way you shape your vocal tract as you close your mouth.</para><para id="eip-166">2) User has to say each vowel for a moderate duration of time. Too long, and a wavering voice would affect success of the program. Too short, and not enough data to assign formants.</para><para id="eip-496">3) This program is calibrated with formant values averaged across all ages and genders. If a male has an exceptionally deep voice, or a child/female has an exceptionally high voice, they may not be able to get accurate vowel readings from the program. Accents may also affect accuracy of results.</para></section></content>

</document>